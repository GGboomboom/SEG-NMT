{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5105)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32,\n",
      " 'beamsize': 5,\n",
      " 'clip_c': 1.0,\n",
      " 'd_maxlen': 200,\n",
      " 'datasets': ['/root/workspace/TMNMT/.dataset/fren.bpe/train.fr.tok.bpe.shuf',\n",
      "              '/root/workspace/TMNMT/.dataset/fren.bpe/train.en.tok.bpe.shuf',\n",
      "              '/root/workspace/TMNMT/.dataset/fren.bpe/train.fr.tok.bpe.shuf',\n",
      "              '/root/workspace/TMNMT/.dataset/fren.bpe/train.en.tok.bpe.shuf'],\n",
      " 'decay_c': 0.0,\n",
      " 'decoder': 'gru_cond',\n",
      " 'dictionaries': ['/root/workspace/TMNMT/.dataset/fren.bpe/train.fr.tok.bpe.pkl',\n",
      "                  '/root/workspace/TMNMT/.dataset/fren.bpe/train.en.tok.bpe.pkl',\n",
      "                  '/root/workspace/TMNMT/.dataset/fren.bpe/train.fr.tok.bpe.pkl',\n",
      "                  '/root/workspace/TMNMT/.dataset/fren.bpe/train.en.tok.bpe.pkl'],\n",
      " 'dim': 1024,\n",
      " 'dim_word': 512,\n",
      " 'dispFreq': 10,\n",
      " 'encoder': 'gru',\n",
      " 'lrate': 2e-05,\n",
      " 'maxlen': 80,\n",
      " 'normalize': False,\n",
      " 'optimizer': 'adam',\n",
      " 'overwrite': False,\n",
      " 'patience': 10,\n",
      " 'reload_': True,\n",
      " 'sampleFreq': 100,\n",
      " 'saveFreq': 100,\n",
      " 'saveto': '/root/workspace/TMNMT/.model/tmv1_fren.bpe.npz',\n",
      " 'trans_from': '/root/workspace/TMNMT/.dataset/fren.bpe/devset.fr.tok.bpe',\n",
      " 'trans_ref': '/root/workspace/TMNMT/.dataset/fren/devset.en.tok',\n",
      " 'trans_to': '/root/workspace/TMNMT/.translate/baseline_fren.bpe.valid',\n",
      " 'use_dropout': False,\n",
      " 'validFreq': 100,\n",
      " 'valid_batch_size': 32,\n",
      " 'valid_datasets': ['/root/workspace/TMNMT/.dataset/fren.bpe/devset.fr.tok.bpe',\n",
      "                    '/root/workspace/TMNMT/.dataset/fren.bpe/devset.en.tok.bpe',\n",
      "                    '/root/workspace/TMNMT/.dataset/fren.bpe/devset.fr.tok.bpe',\n",
      "                    '/root/workspace/TMNMT/.dataset/fren.bpe/devset.en.tok.bpe'],\n",
      " 'voc_sizes': [20000, 20000, 20000, 20000]}\n",
      "Loading data\n",
      "Building model: E -> F & F -> E model\n",
      "Done.\n",
      "build forward-attention models (4 models simultaneously)\n",
      "build_model: elapsed 0.5432 secs.\n",
      "build_model: elapsed 0.2106 secs.\n",
      "build_model: elapsed 0.2078 secs.\n",
      "build_model: elapsed 0.4002 secs.\n",
      "build cross-attention models\n",
      "build_attender: elapsed 0.0991 secs.\n",
      "build_attender: elapsed 0.0937 secs.\n",
      "build_attender: elapsed 0.1017 secs.\n",
      "build_attender: elapsed 0.0956 secs.\n",
      "build attentions (forward, cross-propagation)\n",
      "build loss function (w/o gate)\n",
      "Building Cost Function... Build Gradient (backward)... Done\n",
      "Building Optimizers... Done\n",
      "build_networks: elapsed 773.2813 secs.\n",
      "..Upto here.\n"
     ]
    }
   ],
   "source": [
    "# %load train_nmt.py\n",
    "from nmt import *\n",
    "from pprint import pprint\n",
    "from setup import setup\n",
    "from data_iterator import TextIterator, prepare_data, prepare_cross\n",
    "\n",
    "model_options = setup('fren_bpe')\n",
    "pprint(model_options)\n",
    "\n",
    "# add random seed\n",
    "model_options['trng'] = RandomStreams(19920206)\n",
    "model_options['n_words_src'] = model_options['voc_sizes'][0]\n",
    "model_options['n_words'] = model_options['voc_sizes'][1]\n",
    "\n",
    "# load dictionaries and invert them\n",
    "worddicts   = [None] * len(model_options['dictionaries'])\n",
    "worddicts_r = [None] * len(model_options['dictionaries'])\n",
    "for ii, dd in enumerate(model_options['dictionaries']):\n",
    "    with open(dd, 'rb') as f:\n",
    "        worddicts[ii] = pkl.load(f)\n",
    "    worddicts_r[ii] = dict()\n",
    "    for kk, vv in worddicts[ii].iteritems():\n",
    "        worddicts_r[ii][vv] = kk\n",
    "\n",
    "# reload options\n",
    "if model_options['reload_'] and os.path.exists(model_options['saveto']):\n",
    "    print 'Reloading model options'\n",
    "    with open('%s.pkl' % saveto, 'rb') as f:\n",
    "        model_options = pkl.load(f)\n",
    "\n",
    "print 'Loading data'\n",
    "train = TextIterator(model_options['datasets'], model_options['dictionaries'], model_options['voc_sizes'], \n",
    "                     batch_size=model_options['batch_size'], maxlen=model_options['maxlen'])\n",
    "valid = TextIterator(model_options['valid_datasets'], model_options['dictionaries'], model_options['voc_sizes'], \n",
    "                     batch_size=model_options['batch_size'], maxlen=200)\n",
    "\n",
    "@Timeit\n",
    "def build_networks(options):\n",
    "    funcs = dict()\n",
    "\n",
    "    print 'Building model: E -> F & F -> E model'\n",
    "    params_ef = init_params(options, 'ef_')\n",
    "    params_fe = init_params(options, 'fe_')\n",
    "    print 'Done.'\n",
    "\n",
    "    # reload parameters\n",
    "    if options['reload_'] and os.path.exists(options['saveto']):\n",
    "        print 'Reloading model parameters'\n",
    "        params_ef = load_params(options['saveto'], params_ef)\n",
    "        params_fe = load_params(options['saveto'], params_fe)\n",
    "\n",
    "    tparams_ef = init_tparams(params_ef)\n",
    "    tparams_fe = init_tparams(params_fe)\n",
    "\n",
    "    # inputs of the model (x1, y1, x2, y2)\n",
    "    x1 = tensor.matrix('x1', dtype='int64')\n",
    "    x1_mask = tensor.matrix('x1_mask', dtype='float32')\n",
    "    y1 = tensor.matrix('y1', dtype='int64')\n",
    "    y1_mask = tensor.matrix('y1_mask', dtype='float32')\n",
    "    x2 = tensor.matrix('x2', dtype='int64')\n",
    "    x2_mask = tensor.matrix('x2_mask', dtype='float32')\n",
    "    y2 = tensor.matrix('y2', dtype='int64')\n",
    "    y2_mask = tensor.matrix('y2_mask', dtype='float32')\n",
    "\n",
    "    # TM reference index\n",
    "    tef12 = tensor.matrix('ef12', dtype='int64')\n",
    "    tef12_mask = tensor.matrix('ef12_mask', dtype='float32')\n",
    "    tef21 = tensor.matrix('ef21', dtype='int64')\n",
    "    tef21_mask = tensor.matrix('ef21_mask', dtype='float32')\n",
    "    tfe12 = tensor.matrix('fe12', dtype='int64')\n",
    "    tfe12_mask = tensor.matrix('fe12_mask', dtype='float32')\n",
    "    tfe21 = tensor.matrix('fe21', dtype='int64')\n",
    "    tfe21_mask = tensor.matrix('fe21_mask', dtype='float32')\n",
    "\n",
    "    print 'build forward-attention models (4 models simultaneously)'\n",
    "    ret_ef11 = build_model(tparams_ef, [x1, x1_mask, y1, y1_mask], options, 'ef_', False)  # E->F curr\n",
    "    ret_fe11 = build_model(tparams_fe, [y1, y1_mask, x1, x1_mask], options, 'fe_', False)  # F->E curr\n",
    "    ret_ef22 = build_model(tparams_ef, [x2, x2_mask, y2, y2_mask], options, 'ef_', False)  # E->F tm\n",
    "    ret_fe22 = build_model(tparams_fe, [y2, y2_mask, x2, x2_mask], options, 'fe_', False)  # F->E tm\n",
    "\n",
    "    print 'build cross-attention models'\n",
    "    ret_ef12 = build_attender(tparams_ef,\n",
    "                              [ret_ef11['prev_hids'], ret_ef11['prev_emb'], ret_ef22['ctx'], x2_mask],\n",
    "                              options, 'ef_')  # E->F curr\n",
    "    ret_ef21 = build_attender(tparams_ef,\n",
    "                              [ret_ef22['prev_hids'], ret_ef22['prev_emb'], ret_ef11['ctx'], x1_mask],\n",
    "                              options, 'ef_')  # E->F tm\n",
    "    ret_fe12 = build_attender(tparams_fe,\n",
    "                              [ret_fe11['prev_hids'], ret_fe11['prev_emb'], ret_fe22['ctx'], y2_mask],\n",
    "                              options, 'fe_')  # F->E curr\n",
    "    ret_fe21 = build_attender(tparams_fe,\n",
    "                              [ret_fe22['prev_hids'], ret_fe22['prev_emb'], ret_fe11['ctx'], y1_mask],\n",
    "                              options, 'fe_')  # F->E tm\n",
    "\n",
    "    print 'build attentions (forward, cross-propagation)'\n",
    "\n",
    "    def build_prop(atten_ef, atten_fe):\n",
    "        atten_ef = atten_ef.dimshuffle(1, 0, 2)\n",
    "        atten_fe = atten_fe.dimshuffle(1, 0, 2)\n",
    "        attention = tensor.batched_dot(atten_ef, atten_fe).dimshuffle(1, 0, 2)\n",
    "        return attention\n",
    "\n",
    "    att_ef12 = build_prop(ret_ef12['attention'], ret_fe22['attention'])\n",
    "    att_ef21 = build_prop(ret_ef21['attention'], ret_fe11['attention'])\n",
    "    att_fe12 = build_prop(ret_fe12['attention'], ret_ef22['attention'])\n",
    "    att_fe21 = build_prop(ret_fe21['attention'], ret_ef11['attention'])\n",
    "\n",
    "    print 'build loss function (w/o gate)'\n",
    "\n",
    "    # we first try the simplest version: use a natural attention-gate.\n",
    "    # TODO: make it as a Neural Gate\n",
    "    gate_ef1 = ret_ef11['att_sum'] / (ret_ef11['att_sum'] + ret_ef12['att_sum'])\n",
    "    gate_ef2 = ret_ef22['att_sum'] / (ret_ef22['att_sum'] + ret_ef21['att_sum'])\n",
    "    gate_fe1 = ret_fe11['att_sum'] / (ret_fe11['att_sum'] + ret_fe12['att_sum'])\n",
    "    gate_fe2 = ret_fe22['att_sum'] / (ret_fe22['att_sum'] + ret_fe21['att_sum'])\n",
    "\n",
    "    # get the loss function\n",
    "    def compute_prob(probs, y, y_mask):\n",
    "\n",
    "        # compute the loss for the vocabulary-selection side\n",
    "        y_flat  = y.flatten()\n",
    "        n_words = probs.shape[-1]\n",
    "        y_flat_idx = tensor.arange(y_flat.shape[0]) * n_words + y_flat\n",
    "        probw = probs.flatten()[y_flat_idx]\n",
    "        probw = probw.reshape([y.shape[0], y.shape[1]]) * y_mask\n",
    "        return probw\n",
    "\n",
    "    prob_ef11 = ret_ef11['probs']\n",
    "    prob_ef22 = ret_ef22['probs']\n",
    "    prob_fe11 = ret_fe11['probs']\n",
    "    prob_fe22 = ret_fe22['probs']\n",
    "\n",
    "    # get cost\n",
    "    cost_ef1 = (-tensor.log(compute_prob(prob_ef11, y1, y1_mask) * gate_ef1 +\n",
    "                            compute_prob(att_ef12, tef12, tef12_mask) * (1 - gate_ef1)\n",
    "                            + 1e-8) * (1 - (1 - y1_mask) * (1 - tef12_mask))).sum(0)\n",
    "    cost_ef2 = (-tensor.log(compute_prob(prob_ef22, y2, y2_mask) * gate_ef2 +\n",
    "                            compute_prob(att_ef21, tef21, tef21_mask) * (1 - gate_ef2)\n",
    "                            + 1e-8) * (1 - (1 - y2_mask) * (1 - tef21_mask))).sum(0)\n",
    "    cost_fe1 = (-tensor.log(compute_prob(prob_fe11, x1, x1_mask) * gate_fe1 +\n",
    "                            compute_prob(att_fe12, tfe12, tfe12_mask) * (1 - gate_fe1)\n",
    "                            + 1e-8) * (1 - (1 - x1_mask) * (1 - tfe12_mask))).sum(0)\n",
    "    cost_fe2 = (-tensor.log(compute_prob(prob_fe22, x2, x2_mask) * gate_fe2 +\n",
    "                            compute_prob(att_fe21, tfe21, tfe21_mask) * (1 - gate_fe2)\n",
    "                            + 1e-8) * (1 - (1 - x2_mask) * (1 - tfe21_mask))).sum(0)\n",
    "\n",
    "    cost = cost_ef1 + cost_ef2 + cost_fe1 + cost_fe2\n",
    "\n",
    "    # print 'Building sampler'\n",
    "    # f_init, f_next = build_sampler(tparams, options, trng, use_noise)\n",
    "\n",
    "    # before any regularizer\n",
    "    print 'Building Cost Function...',\n",
    "    inputs = [x1, x1_mask, y1, y1_mask, x2, x2_mask, y2, y2_mask,\n",
    "              tef12, tef12_mask, tef21, tef21_mask,\n",
    "              tfe12, tfe12_mask, tfe21, tfe21_mask]\n",
    "\n",
    "    # f_cost = theano.function(inputs, cost, profile=profile)\n",
    "    # print 'Done'\n",
    "\n",
    "    cost = cost.mean()\n",
    "\n",
    "    print 'Build Gradient (backward)...',\n",
    "\n",
    "    tparams = dict(tparams_ef.items() + tparams_fe.items())\n",
    "    grads   = clip(tensor.grad(cost, wrt=itemlist(tparams)), options['clip_c'])\n",
    "    print 'Done'\n",
    "\n",
    "    # compile the optimizer, the actual computational graph is compiled here\n",
    "    lr = tensor.scalar(name='lr')\n",
    "    print 'Building Optimizers...',\n",
    "    f_cost, f_update = eval(options['optimizer'])(lr, tparams, grads, inputs, cost)\n",
    "\n",
    "    funcs['cost']   = f_cost\n",
    "    funcs['update'] = f_update\n",
    "\n",
    "    print 'Done'\n",
    "    return funcs\n",
    "\n",
    "funcs = build_networks(model_options)\n",
    "\n",
    "print '..Upto here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "0 0 0 0 \n",
      "(81, 16) (81, 16) (79, 16) (79, 16)\n",
      "cost= 1397.23193359\n",
      "update []\n",
      "286 286 286 286 \n",
      "(79, 16) (79, 16) (59, 16) (59, 16)\n",
      "cost= 1087.9440918\n",
      "update []\n",
      "265 265 265 265 \n",
      "(73, 16) (73, 16) (48, 16) (48, 16)\n",
      "cost= 944.034423828\n",
      "update []\n",
      "249 249 249 249 \n",
      "(66, 16) (66, 16) (43, 16) (43, 16)\n",
      "cost= 968.951171875\n",
      "update []\n",
      "233 233 233 233 \n",
      "(65, 16) (65, 16) (40, 16) (40, 16)\n",
      "cost= 949.725769043\n",
      "update []\n",
      "217 217 217 217 \n",
      "(59, 16) (59, 16) (36, 16) (36, 16)\n",
      "cost= 635.390563965\n",
      "update []\n",
      "201 201 201 201 \n",
      "(54, 16) (54, 16) (33, 16) (33, 16)\n",
      "cost= 563.780212402\n",
      "update []\n",
      "185 185 185 185 \n",
      "(47, 16) (47, 16) (31, 16) (31, 16)\n",
      "cost= 515.678955078\n",
      "update []\n",
      "169 169 169 169 \n",
      "(43, 16) (43, 16) (27, 16) (27, 16)\n",
      "cost= 439.389862061\n",
      "update []\n",
      "153 153 153 153 \n",
      "(41, 16) (41, 16) (26, 16) (26, 16)\n",
      "cost= 387.935913086\n",
      "update []\n",
      "137 137 137 137 \n",
      "(33, 16) (33, 16) (24, 16) (24, 16)\n",
      "cost= 346.955718994\n",
      "update []\n",
      "121 121 121 121 \n",
      "(33, 16) (33, 16) (22, 16) (22, 16)\n",
      "cost= 319.264923096\n",
      "update []\n",
      "105 105 105 105 \n",
      "(31, 16) (31, 16) (20, 16) (20, 16)\n",
      "cost= 274.329376221\n",
      "update []\n",
      "89 89 89 89 \n",
      "(24, 16) (24, 16) (17, 16) (17, 16)\n",
      "cost= 223.297119141\n",
      "update []\n",
      "73 73 73 73 \n",
      "(25, 16) (25, 16) (14, 16) (14, 16)\n",
      "cost= 179.599151611\n",
      "update []\n",
      "57 57 57 57 \n",
      "(35, 16) (35, 16) (11, 16) (11, 16)\n",
      "cost= 155.035766602\n",
      "update []\n",
      "41 41 41 41 \n",
      "(14, 16) (14, 16) (9, 16) (9, 16)\n",
      "cost= 99.8120727539\n",
      "update []\n",
      "25 25 25 25 \n",
      "(15, 16) (15, 16) (7, 16) (7, 16)\n",
      "cost= 75.8618011475\n",
      "update []\n",
      "9 9 9 9 \n",
      "(81, 16) (81, 16) (75, 16) (75, 16)\n",
      "cost= 584.26373291\n",
      "update []\n",
      "300 300 300 300 \n",
      "(81, 16) (81, 16) (66, 16) (66, 16)\n",
      "cost= 1136.12597656\n",
      "update []\n",
      "276 276 276 276 \n",
      "(80, 16) (80, 16) (58, 16) (58, 16)\n",
      "cost= 1009.15246582\n",
      "update []\n",
      "259 259 259 259 \n",
      "(79, 16) (79, 16) (51, 16) (51, 16)\n",
      "cost= 863.159057617\n",
      "update []\n",
      "242 242 242 242 \n",
      "(73, 16) (73, 16) (47, 16) (47, 16)\n",
      "cost= 775.04699707\n",
      "update []\n",
      "226 226 226 226 \n",
      "(61, 16) (61, 16) (44, 16) (44, 16)\n",
      "cost= 735.782287598\n",
      "update []\n",
      "210 210 210 210 \n",
      "(64, 16) (64, 16) (41, 16) (41, 16)\n",
      "cost= 692.526123047\n",
      "update []\n",
      "194 194 194 194 \n",
      "(62, 16) (62, 16) (37, 16) (37, 16)\n",
      "cost= 594.479492188\n",
      "update []\n",
      "178 178 178 178 \n",
      "(68, 16) (68, 16) (34, 16) (34, 16)\n",
      "cost= 550.12890625\n",
      "update []\n",
      "162 162 162 162 \n",
      "(47, 16) (47, 16) (31, 16) (31, 16)\n",
      "cost= 460.299163818\n",
      "update []\n",
      "146 146 146 146 \n",
      "(38, 16) (38, 16) (29, 16) (29, 16)\n",
      "cost= 397.039489746\n",
      "update []\n",
      "130 130 130 130 \n",
      "(39, 16) (39, 16) (27, 16) (27, 16)\n",
      "cost= 359.339477539\n",
      "update []\n",
      "114 114 114 114 \n",
      "(39, 16) (39, 16) (24, 16) (24, 16)\n",
      "cost= 337.467834473\n",
      "update []\n",
      "98 98 98 98 \n",
      "(39, 16) (39, 16) (22, 16) (22, 16)\n",
      "cost= 295.428161621\n",
      "update []\n",
      "82 82 82 82 \n",
      "(30, 16) (30, 16) (19, 16) (19, 16)\n",
      "cost= 250.8956604\n",
      "update []\n",
      "66 66 66 66 \n",
      "(26, 16) (26, 16) (17, 16) (17, 16)\n",
      "cost= 219.819290161\n",
      "update []\n",
      "50 50 50 50 \n",
      "(20, 16) (20, 16) (14, 16) (14, 16)\n",
      "cost= 157.145355225\n",
      "update []\n",
      "34 34 34 34 \n",
      "(20, 16) (20, 16) (10, 16) (10, 16)\n",
      "cost= 118.062042236\n",
      "update []\n",
      "18 18 18 18 \n",
      "(16, 16) (16, 16) (8, 16) (8, 16)\n",
      "cost= 76.9487609863\n",
      "update []\n",
      "2 2 2 2 \n",
      "(81, 16) (81, 16) (73, 16) (73, 16)\n",
      "cost= 1026.81518555\n",
      "update []\n",
      "286 286 286 286 \n",
      "(79, 16) (79, 16) (56, 16) (56, 16)\n",
      "cost= 963.561950684\n",
      "update []\n",
      "269 269 269 269 \n",
      "(75, 16) (75, 16) (49, 16) (49, 16)\n",
      "cost= 806.505554199\n",
      "update []\n",
      "253 253 253 253 \n",
      "(69, 16) (69, 16) (44, 16) (44, 16)\n",
      "cost= 717.166564941\n",
      "update []\n",
      "237 237 237 237 \n",
      "(58, 16) (58, 16) (40, 16) (40, 16)\n",
      "cost= 630.410766602\n",
      "update []\n",
      "221 221 221 221 \n",
      "(50, 16) (50, 16) (38, 16) (38, 16)\n",
      "cost= 581.477905273\n",
      "update []\n",
      "205 205 205 205 \n",
      "(54, 16) (54, 16) (34, 16) (34, 16)\n",
      "cost= 540.127319336\n",
      "update []\n",
      "189 189 189 189 \n",
      "(46, 16) (46, 16) (30, 16) (30, 16)\n",
      "cost= 433.30456543\n",
      "update []\n",
      "173 173 173 173 \n",
      "(41, 16) (41, 16) (27, 16) (27, 16)\n",
      "cost= 391.586730957\n",
      "update []\n",
      "157 157 157 157 \n",
      "(47, 16) (47, 16) (25, 16) (25, 16)\n",
      "cost= 350.279296875\n",
      "update []\n",
      "141 141 141 141 \n",
      "(28, 16) (28, 16) (23, 16) (23, 16)\n",
      "cost= 300.867706299\n",
      "update []\n",
      "125 125 125 125 \n",
      "(30, 16) (30, 16) (21, 16) (21, 16)\n",
      "cost= 279.185913086\n",
      "update []\n",
      "109 109 109 109 \n",
      "(32, 16) (32, 16) (19, 16) (19, 16)\n",
      "cost= 241.638977051\n",
      "update []\n",
      "93 93 93 93 \n",
      "(26, 16) (26, 16) (16, 16) (16, 16)\n",
      "cost= 221.987884521\n",
      "update []\n",
      "77 77 77 77 \n",
      "(19, 16) (19, 16) (14, 16) (14, 16)\n",
      "cost= 167.543334961\n",
      "update []\n",
      "61 61 61 61 \n",
      "(20, 16) (20, 16) (12, 16) (12, 16)\n",
      "cost= 154.95300293\n",
      "update []\n",
      "45 45 45 45 \n",
      "(16, 16) (16, 16) (10, 16) (10, 16)\n",
      "cost= 109.11857605\n",
      "update []\n",
      "29 29 29 29 \n",
      "(15, 16) (15, 16) (8, 16) (8, 16)\n",
      "cost= 87.173828125\n",
      "update []\n",
      "13 13 13 13 \n",
      "(81, 16) (81, 16) (72, 16) (72, 16)\n",
      "cost= 266.982086182\n",
      "update []\n",
      "299 299 299 299 \n",
      "(81, 16) (81, 16) (68, 16) (68, 16)\n",
      "cost= 1175.9119873\n",
      "update []\n",
      "281 281 281 281 \n",
      "(79, 16) (79, 16) (60, 16) (60, 16)\n",
      "cost= 1037.16333008\n",
      "update []\n",
      "264 264 264 264 \n",
      "(73, 16) (73, 16) (53, 16) (53, 16)\n",
      "cost= 851.976806641\n",
      "update []\n",
      "247 247 247 247 \n",
      "(75, 16) (75, 16) (47, 16) (47, 16)\n",
      "cost= 844.748413086\n",
      "update []\n",
      "231 231 231 231 \n",
      "(60, 16) (60, 16) (43, 16) (43, 16)\n",
      "cost= 699.510009766\n",
      "update []\n",
      "215 215 215 215 \n",
      "(60, 16) (60, 16) (39, 16) (39, 16)\n",
      "cost= 647.398193359\n",
      "update []\n",
      "199 199 199 199 \n",
      "(56, 16) (56, 16) (37, 16) (37, 16)\n",
      "cost= 573.372375488\n",
      "update []\n",
      "183 183 183 183 \n",
      "(54, 16) (54, 16) (33, 16) (33, 16)\n",
      "cost= 507.20135498\n",
      "update []\n",
      "167 167 167 167 \n",
      "(45, 16) (45, 16) (29, 16) (29, 16)\n",
      "cost= 446.346740723\n",
      "update []\n",
      "151 151 151 151 \n",
      "(42, 16) (42, 16) (26, 16) (26, 16)\n",
      "cost= 356.457122803\n",
      "update []\n",
      "135 135 135 135 \n",
      "(42, 16) (42, 16) (23, 16) (23, 16)\n",
      "cost= 317.670349121\n",
      "update []\n",
      "119 119 119 119 \n",
      "(34, 16) (34, 16) (22, 16) (22, 16)\n",
      "cost= 294.757049561\n",
      "update []\n",
      "103 103 103 103 \n",
      "(35, 16) (35, 16) (21, 16) (21, 16)\n",
      "cost= 271.046142578\n",
      "update []\n",
      "87 87 87 87 \n",
      "(28, 16) (28, 16) (18, 16) (18, 16)\n",
      "cost= 230.099121094\n",
      "update []\n",
      "71 71 71 71 \n",
      "(27, 16) (27, 16) (16, 16) (16, 16)\n",
      "cost= 205.505615234\n",
      "update []\n",
      "55 55 55 55 \n",
      "(24, 16) (24, 16) (13, 16) (13, 16)\n",
      "cost= 162.595550537\n",
      "update []\n",
      "39 39 39 39 \n",
      "(16, 16) (16, 16) (11, 16) (11, 16)\n",
      "cost= 113.258026123\n",
      "update []\n",
      "23 23 23 23 \n",
      "(16, 16) (16, 16) (8, 16) (8, 16)\n",
      "cost= 72.1748428345\n",
      "update []\n",
      "7 7 7 7 \n",
      "(80, 16) (80, 16) (79, 16) (79, 16)\n",
      "cost= 695.41796875\n",
      "update []\n",
      "296 296 296 296 \n",
      "(81, 16) (81, 16) (68, 16) (68, 16)\n",
      "cost= 1048.26208496\n",
      "update []\n",
      "269 269 269 269 \n",
      "(78, 16) (78, 16) (60, 16) (60, 16)\n",
      "cost= 1041.50439453\n",
      "update []\n",
      "251 251 251 251 \n",
      "(78, 16) (78, 16) (52, 16) (52, 16)\n",
      "cost= 877.625732422\n",
      "update []\n",
      "234 234 234 234 \n",
      "(70, 16) (70, 16) (48, 16) (48, 16)\n",
      "cost= 775.91986084\n",
      "update []\n",
      "218 218 218 218 \n",
      "(73, 16) (73, 16) (44, 16) (44, 16)\n",
      "cost= 748.645812988\n",
      "update []\n",
      "202 202 202 202 \n",
      "(67, 16) (67, 16) (40, 16) (40, 16)\n",
      "cost= 651.780944824\n",
      "update []\n",
      "186 186 186 186 \n",
      "(50, 16) (50, 16) (36, 16) (36, 16)\n",
      "cost= 557.114990234\n",
      "update []\n",
      "170 170 170 170 \n",
      "(58, 16) (58, 16) (33, 16) (33, 16)\n",
      "cost= 497.183013916\n",
      "update []\n",
      "154 154 154 154 \n",
      "(43, 16) (43, 16) (31, 16) (31, 16)\n",
      "cost= 434.655395508\n",
      "update []\n",
      "138 138 138 138 \n",
      "(49, 16) (49, 16) (28, 16) (28, 16)\n",
      "cost= 398.160095215\n",
      "update []\n",
      "122 122 122 122 \n",
      "(50, 16) (50, 16) (25, 16) (25, 16)\n",
      "cost= 349.160095215\n",
      "update []\n",
      "106 106 106 106 \n",
      "(33, 16) (33, 16) (23, 16) (23, 16)\n",
      "cost= 295.121948242\n",
      "update []\n",
      "90 90 90 90 \n",
      "(31, 16) (31, 16) (21, 16) (21, 16)\n",
      "cost= 255.552215576\n",
      "update []\n",
      "74 74 74 74 \n",
      "(36, 16) (36, 16) (18, 16) (18, 16)\n",
      "cost= 238.310592651\n",
      "update []\n",
      "58 58 58 58 \n",
      "(21, 16) (21, 16) (14, 16) (14, 16)\n",
      "cost= 169.390289307\n",
      "update []\n",
      "42 42 42 42 \n",
      "(16, 16) (16, 16) (11, 16) (11, 16)\n",
      "cost= 125.597236633\n",
      "update []\n",
      "26 26 26 26 \n",
      "(14, 16) (14, 16) (8, 16) (8, 16)\n",
      "cost= 78.7705535889\n",
      "update []\n",
      "10 10 10 10 \n",
      "(81, 16) (81, 16) (78, 16) (78, 16)\n",
      "cost= 502.627929688\n",
      "update []\n",
      "298 298 298 298 \n",
      "(81, 16) (81, 16) (66, 16) (66, 16)\n",
      "cost= 1142.83728027\n",
      "update []\n",
      "278 278 278 278 \n",
      "(79, 16) (79, 16) (58, 16) (58, 16)\n",
      "cost= 1023.20318604\n",
      "update []\n",
      "261 261 261 261 \n",
      "(79, 16) (79, 16) (53, 16) (53, 16)\n",
      "cost= 913.270446777\n",
      "update []\n",
      "243 243 243 243 \n",
      "(81, 16) (81, 16) (47, 16) (47, 16)\n",
      "cost= 835.575500488\n",
      "update []\n",
      "227 227 227 227 \n",
      "(60, 16) (60, 16) (44, 16) (44, 16)\n",
      "cost= 660.318725586\n",
      "update []\n",
      "211 211 211 211 \n",
      "(65, 16) (65, 16) (39, 16) (39, 16)\n",
      "cost= 646.733703613\n",
      "update []\n",
      "195 195 195 195 \n",
      "(54, 16) (54, 16) (36, 16) (36, 16)\n",
      "cost= 551.827270508\n",
      "update []\n",
      "179 179 179 179 \n",
      "(54, 16) (54, 16) (32, 16) (32, 16)\n",
      "cost= 494.27722168\n",
      "update []\n",
      "163 163 163 163 \n",
      "(42, 16) (42, 16) (28, 16) (28, 16)\n",
      "cost= 412.867004395\n",
      "update []\n",
      "147 147 147 147 \n",
      "(42, 16) (42, 16) (26, 16) (26, 16)\n",
      "cost= 357.074462891\n",
      "update []\n",
      "131 131 131 131 \n",
      "(56, 16) (56, 16) (23, 16) (23, 16)\n",
      "cost= 340.83807373\n",
      "update []\n",
      "115 115 115 115 \n",
      "(41, 16) (41, 16) (22, 16) (22, 16)\n",
      "cost= 312.500610352\n",
      "update []\n",
      "99 99 99 99 \n",
      "(41, 16) (41, 16) (19, 16) (19, 16)\n",
      "cost= 247.20211792\n",
      "update []\n",
      "83 83 83 83 \n",
      "(27, 16) (27, 16) (16, 16) (16, 16)\n",
      "cost= 206.291656494\n",
      "update []\n",
      "67 67 67 67 \n",
      "(22, 16) (22, 16) (13, 16) (13, 16)\n",
      "cost= 142.123413086\n",
      "update []\n",
      "51 51 51 51 \n",
      "(15, 16) (15, 16) (10, 16) (10, 16)\n",
      "cost= 115.837417603\n",
      "update []\n",
      "35 35 35 35 \n",
      "(13, 16) (13, 16) (8, 16) (8, 16)\n",
      "cost= 81.4658203125\n",
      "update []\n",
      "19 19 19 19 \n",
      "(10, 16) (10, 16) (6, 16) (6, 16)\n",
      "cost= 52.84897995\n",
      "update []\n",
      "3 3 3 3 \n",
      "(79, 16) (79, 16) (75, 16) (75, 16)\n",
      "cost= 920.650268555\n",
      "update []\n",
      "289 289 289 289 \n",
      "(80, 16) (80, 16) (61, 16) (61, 16)\n",
      "cost= 1088.36035156\n",
      "update []\n",
      "270 270 270 270 \n",
      "(76, 16) (76, 16) (54, 16) (54, 16)\n",
      "cost= 869.504882812\n",
      "update []\n",
      "254 254 254 254 \n",
      "(80, 16) (80, 16) (48, 16) (48, 16)\n",
      "cost= 815.769348145\n",
      "update []\n",
      "238 238 238 238 \n",
      "(69, 16) (69, 16) (45, 16) (45, 16)\n",
      "cost= 740.387451172\n",
      "update []\n",
      "222 222 222 222 \n",
      "(66, 16) (66, 16) (41, 16) (41, 16)\n",
      "cost= 653.319213867\n",
      "update []\n",
      "206 206 206 206 \n",
      "(62, 16) (62, 16) (37, 16) (37, 16)\n",
      "cost= 610.519897461\n",
      "update []\n",
      "190 190 190 190 \n",
      "(48, 16) (48, 16) (35, 16) (35, 16)\n",
      "cost= 507.762145996\n",
      "update []\n",
      "174 174 174 174 \n",
      "(49, 16) (49, 16) (31, 16) (31, 16)\n",
      "cost= 460.231842041\n",
      "update []\n",
      "158 158 158 158 \n",
      "(53, 16) (53, 16) (28, 16) (28, 16)\n",
      "cost= 380.962219238\n",
      "update []\n",
      "142 142 142 142 \n",
      "(41, 16) (41, 16) (25, 16) (25, 16)\n",
      "cost= 318.241455078\n",
      "update []\n",
      "126 126 126 126 \n",
      "(41, 16) (41, 16) (22, 16) (22, 16)\n",
      "cost= 297.377624512\n",
      "update []\n",
      "110 110 110 110 \n",
      "(34, 16) (34, 16) (20, 16) (20, 16)\n",
      "cost= 276.492126465\n",
      "update []\n",
      "94 94 94 94 \n",
      "(26, 16) (26, 16) (18, 16) (18, 16)\n",
      "cost= 230.089401245\n",
      "update []\n",
      "78 78 78 78 \n",
      "(25, 16) (25, 16) (16, 16) (16, 16)\n",
      "cost= 198.57901001\n",
      "update []\n",
      "62 62 62 62 \n",
      "(22, 16) (22, 16) (14, 16) (14, 16)\n",
      "cost= 170.110061646\n",
      "update []\n",
      "46 46 46 46 \n",
      "(15, 16) (15, 16) (12, 16) (12, 16)\n",
      "cost= 125.763549805\n",
      "update []\n",
      "30 30 30 30 \n",
      "(15, 16) (15, 16) (9, 16) (9, 16)\n",
      "cost= 94.7208938599\n",
      "update []\n",
      "14 14 14 14 \n",
      "(80, 16) (80, 16) (76, 16) (76, 16)\n",
      "cost= 202.613555908\n",
      "update []\n",
      "308 308 308 308 \n",
      "(81, 16) (81, 16) (70, 16) (70, 16)\n",
      "cost= 1150.60876465\n",
      "update []\n",
      "282 282 282 282 \n",
      "(81, 16) (81, 16) (58, 16) (58, 16)\n",
      "cost= 980.058227539\n",
      "update []\n",
      "265 265 265 265 \n",
      "(78, 16) (78, 16) (53, 16) (53, 16)\n",
      "cost= 905.287719727\n",
      "update []\n",
      "249 249 249 249 \n",
      "(71, 16) (71, 16) (47, 16) (47, 16)\n",
      "cost= 716.373657227\n",
      "update []\n",
      "233 233 233 233 \n",
      "(55, 16) (55, 16) (42, 16) (42, 16)\n",
      "cost= 634.666381836\n",
      "update []\n",
      "217 217 217 217 \n",
      "(55, 16) (55, 16) (38, 16) (38, 16)\n",
      "cost= 570.517944336\n",
      "update []\n",
      "201 201 201 201 \n",
      "(46, 16) (46, 16) (35, 16) (35, 16)\n",
      "cost= 505.937255859\n",
      "update []\n",
      "185 185 185 185 \n",
      "(63, 16) (63, 16) (31, 16) (31, 16)\n",
      "cost= 486.740478516\n",
      "update []\n",
      "169 169 169 169 \n",
      "(42, 16) (42, 16) (29, 16) (29, 16)\n",
      "cost= 414.071777344\n",
      "update []\n",
      "153 153 153 153 \n",
      "(42, 16) (42, 16) (26, 16) (26, 16)\n",
      "cost= 352.729064941\n",
      "update []\n",
      "137 137 137 137 \n",
      "(33, 16) (33, 16) (24, 16) (24, 16)\n",
      "cost= 317.436035156\n",
      "update []\n",
      "121 121 121 121 \n",
      "(32, 16) (32, 16) (22, 16) (22, 16)\n",
      "cost= 279.225585938\n",
      "update []\n",
      "105 105 105 105 \n",
      "(37, 16) (37, 16) (19, 16) (19, 16)\n",
      "cost= 238.146438599\n",
      "update []\n",
      "89 89 89 89 \n",
      "(28, 16) (28, 16) (16, 16) (16, 16)\n",
      "cost= 195.783203125\n",
      "update []\n",
      "73 73 73 73 \n",
      "(22, 16) (22, 16) (13, 16) (13, 16)\n",
      "cost= 174.925292969\n",
      "update []\n",
      "57 57 57 57 \n",
      "(19, 16) (19, 16) (11, 16) (11, 16)\n",
      "cost= 135.85736084\n",
      "update []\n",
      "41 41 41 41 \n",
      "(13, 16) (13, 16) (9, 16) (9, 16)\n",
      "cost= 91.6189041138\n",
      "update []\n",
      "25 25 25 25 \n",
      "(12, 16) (12, 16) (7, 16) (7, 16)\n",
      "cost= 64.3146362305\n",
      "update []\n",
      "9 9 9 9 \n",
      "(81, 16) (81, 16) (76, 16) (76, 16)\n",
      "cost= 520.130737305\n",
      "update []\n",
      "303 303 303 303 \n",
      "(80, 16) (80, 16) (67, 16) (67, 16)\n",
      "cost= 1100.80310059\n",
      "update []\n",
      "285 285 285 285 \n",
      "(80, 16) (80, 16) (57, 16) (57, 16)\n",
      "cost= 973.002685547\n",
      "update []\n",
      "268 268 268 268 \n",
      "(74, 16) (74, 16) (53, 16) (53, 16)\n",
      "cost= 928.381591797\n",
      "update []\n",
      "252 252 252 252 \n",
      "(64, 16) (64, 16) (48, 16) (48, 16)\n",
      "cost= 722.45324707\n",
      "update []\n",
      "236 236 236 236 \n",
      "(66, 16) (66, 16) (43, 16) (43, 16)\n",
      "cost= 721.87677002\n",
      "update []\n",
      "219 219 219 219 \n",
      "(59, 16) (59, 16) (40, 16) (40, 16)\n",
      "cost= 646.805419922\n",
      "update []\n",
      "203 203 203 203 \n",
      "(59, 16) (59, 16) (38, 16) (38, 16)\n",
      "cost= 593.764038086\n",
      "update []\n",
      "187 187 187 187 \n",
      "(49, 16) (49, 16) (36, 16) (36, 16)\n",
      "cost= 513.885620117\n",
      "update []\n",
      "171 171 171 171 \n",
      "(43, 16) (43, 16) (32, 16) (32, 16)\n",
      "cost= 452.990814209\n",
      "update []\n",
      "155 155 155 155 \n",
      "(47, 16) (47, 16) (29, 16) (29, 16)\n",
      "cost="
     ]
    }
   ],
   "source": [
    "print 'Loading data'\n",
    "train = TextIterator(model_options['datasets'], model_options['dictionaries'], model_options['voc_sizes'], \n",
    "                     batch_size=16, maxlen=model_options['maxlen'])\n",
    "valid = TextIterator(model_options['valid_datasets'], model_options['dictionaries'], model_options['voc_sizes'], \n",
    "                     batch_size=model_options['batch_size'], maxlen=200)\n",
    "train.reset()\n",
    "lrate = model_options['lrate']\n",
    "for k, (sx1, sy1, sx2, sy2) in enumerate(train):\n",
    "    x1, x1_mask = prepare_data(sx1, model_options['maxlen'], model_options['voc_sizes'][0])\n",
    "    y1, y1_mask = prepare_data(sy1, model_options['maxlen'], model_options['voc_sizes'][1])\n",
    "    x2, x2_mask = prepare_data(sx2, model_options['maxlen'], model_options['voc_sizes'][2])\n",
    "    y2, y2_mask = prepare_data(sy2, model_options['maxlen'], model_options['voc_sizes'][3])\n",
    "    \n",
    "    print x1.shape, x2.shape, y1.shape, y2.shape\n",
    "    tx12, tx12_mask = prepare_cross(sx1, sx2, x1.shape[0])\n",
    "    tx21, tx21_mask = prepare_cross(sx2, sx1, x2.shape[0])\n",
    "    ty12, ty12_mask = prepare_cross(sy1, sy2, y1.shape[0])\n",
    "    ty21, ty21_mask = prepare_cross(sy1, sy2, y2.shape[0])\n",
    "    \n",
    "    inps = [x1, x1_mask, y1, y1_mask, \n",
    "            x2, x2_mask, y2, y2_mask,\n",
    "            ty12, ty12_mask, ty21, ty21_mask,\n",
    "            tx12, tx12_mask, tx21, tx21_mask]\n",
    "    \n",
    "    if k > 200:\n",
    "        break\n",
    "    print 'cost=', funcs['cost'](*inps)\n",
    "    print 'update', funcs['update'](lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16832   197   218    57  1709  1972    14    47     3   414    68     3\n",
      "    47    40   418 17854   147 11917    20   294    20   290    95    40\n",
      "   143     3  5490]\n"
     ]
    }
   ],
   "source": [
    "print x2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "a = [23, 1,2,32,23, 23,4]\n",
    "b = [23, 2,10,23]\n",
    "idx = [[(i, abs(i-j)) for i, x in enumerate(a) if x == y] for j, y in enumerate(b)]\n",
    "print sorted(idx[3], key=lambda a:a[1])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}